{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNle2c3xwcJlMUGpEIcr35g"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Main Project file\n",
        "needs custom path to a source file to run"
      ],
      "metadata": {
        "id": "QlyhEnXW6e4o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyxgFbq96a7X",
        "outputId": "b1599cce-8390-458f-eb38-93747385c181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@startuml\n",
            "class ĠItem {\n",
            "  rate\n",
            "  Ġall\n",
            "  Ġquantity\n",
            "  Ġprice\n",
            "  Ġquantity\n",
            "  name\n",
            "  price\n",
            "  ity\n",
            "  price\n",
            "  Ġreader\n",
            "  Ġitems\n",
            "  Ġname\n",
            "  Ġprice\n",
            "  Ġquantity\n",
            "}\n",
            "class method {\n",
            "  rate\n",
            "  Ġall\n",
            "  Ġquantity\n",
            "  Ġprice\n",
            "  Ġquantity\n",
            "  name\n",
            "  price\n",
            "  ity\n",
            "  price\n",
            "  Ġreader\n",
            "  Ġitems\n",
            "  Ġname\n",
            "  Ġprice\n",
            "  Ġquantity\n",
            "}\n",
            "@enduml\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "class CodeAIAnalyser:\n",
        "    def __init__(self, model_name=\"microsoft/codebert-base\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    def analyze_code(self, text):\n",
        "        # Tokenize the input text and get model outputs\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        outputs = self.model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state\n",
        "        return embeddings, inputs['input_ids']\n",
        "\n",
        "    def extract_features(self, embeddings, input_ids):\n",
        "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
        "        features = {\n",
        "            \"classes\": [],\n",
        "            \"methods\": [],\n",
        "            \"attributes\": []\n",
        "        }\n",
        "\n",
        "        current_class = None\n",
        "        for i, token in enumerate(tokens):\n",
        "            if token.startswith(\"class\") and (i + 1 < len(tokens)):\n",
        "                class_name = tokens[i + 1]\n",
        "                current_class = class_name\n",
        "                features[\"classes\"].append(class_name)\n",
        "            elif token.startswith(\"def\") and current_class:\n",
        "                method_name = tokens[i + 1] + \"()\"\n",
        "                features[\"methods\"].append(method_name)\n",
        "            elif \"=\" in token and current_class:\n",
        "                attribute_name = tokens[i - 1]\n",
        "                features[\"attributes\"].append(attribute_name)\n",
        "\n",
        "        return features\n",
        "\n",
        "def generate_uml(features):\n",
        "    uml_text = \"@startuml\\n\"\n",
        "    for cls in features[\"classes\"]:\n",
        "        uml_text += f\"class {cls} {{\\n\"\n",
        "        for attr in features[\"attributes\"]:\n",
        "            uml_text += f\"  {attr}\\n\"\n",
        "        for method in features[\"methods\"]:\n",
        "            uml_text += f\"  {method}\\n\"\n",
        "        uml_text += \"}\\n\"\n",
        "    uml_text += \"@enduml\"\n",
        "    return uml_text\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    analyser = CodeAIAnalyser()\n",
        "    file_path = '/content/drive/My Drive/PythonOOP-main/05 - Class Inheritance/main.py'\n",
        "    with open(file_path, 'r') as file:\n",
        "        code_text = file.read()\n",
        "    embeddings,input_ids = analyser.analyze_code(code_text)\n",
        "    features = analyser.extract_features(embeddings, input_ids)\n",
        "    uml_text = generate_uml(features)\n",
        "    print(uml_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All code below contains different versions of the above.\n",
        "This one doesn't use AI and only takes in Python files."
      ],
      "metadata": {
        "id": "XmE_Q6y56x5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import ast\n",
        "import logging\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class UMLClass:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.methods = []\n",
        "        self.attributes = []\n",
        "        self.inheritances = []\n",
        "\n",
        "    def add_method(self, method_name):\n",
        "        self.methods.append(method_name)\n",
        "\n",
        "    def add_attribute(self, attribute_name):\n",
        "        self.attributes.append(attribute_name)\n",
        "\n",
        "    def set_inheritance(self, parent_class):\n",
        "        self.inheritances.append(parent_class)\n",
        "\n",
        "class CodeAnalyser:\n",
        "    def __init__(self, root_directory):\n",
        "        self.root_directory = root_directory\n",
        "\n",
        "    def list_files(self):\n",
        "        excluded_directories = ['.git', '.idea', '.settings', '.mvn', 'target']\n",
        "        excluded_file_types = ['.gitattributes', '.md', '.class', '.pyc']\n",
        "        file_list = []\n",
        "        for root, dirs, files in os.walk(self.root_directory):\n",
        "            dirs[:] = [d for d in dirs if d not in excluded_directories]\n",
        "            for file_name in files:\n",
        "                if not any(file_name.endswith(ext) for ext in excluded_file_types):\n",
        "                    file_path = os.path.join(root, file_name)\n",
        "                    file_list.append(file_path)\n",
        "        return file_list\n",
        "\n",
        "    def parse_python_file(self, file_path):\n",
        "        with open(file_path, 'r') as file:\n",
        "            node = ast.parse(file.read(), filename=file_path)\n",
        "        classes = []\n",
        "        for elem in node.body:\n",
        "            if isinstance(elem, ast.ClassDef):\n",
        "                new_class = UMLClass(elem.name)\n",
        "                for base in elem.bases:\n",
        "                    new_class.set_inheritance(base.id)\n",
        "                for item in elem.body:\n",
        "                    if isinstance(item, ast.FunctionDef):\n",
        "                        new_class.add_method(item.name)\n",
        "                    elif isinstance(item, ast.Assign):\n",
        "                        for target in item.targets:\n",
        "                            if isinstance(target, ast.Name):\n",
        "                                new_class.add_attribute(target.id)\n",
        "                classes.append(new_class)\n",
        "        return classes\n",
        "\n",
        "    def generate_plantuml(self, classes):\n",
        "        diagram = \"@startuml\\n\"\n",
        "        for cls in classes:\n",
        "            diagram += f\"class {cls.name} {{\\n\"\n",
        "            for attr in cls.attributes:\n",
        "                diagram += f\"  {attr}\\n\"\n",
        "            for method in cls.methods:\n",
        "                diagram += f\"  {method}()\\n\"\n",
        "            diagram += \"}\\n\"\n",
        "        for cls in classes:\n",
        "            for parent in cls.inheritances:\n",
        "                diagram += f\"{parent} <|-- {cls.name}\\n\"\n",
        "        diagram += \"@enduml\"\n",
        "        return diagram\n",
        "\n",
        "    def analyze_and_generate_uml(self):\n",
        "        files = self.list_files()\n",
        "        uml_classes = []\n",
        "        for file_path in files:\n",
        "            class_info = self.parse_python_file(file_path)\n",
        "            uml_classes.extend(class_info)\n",
        "        uml_diagram = self.generate_plantuml(uml_classes)\n",
        "        return uml_diagram\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    analyser = CodeAnalyser('/content/drive/My Drive/Java-OOP-master/Inheritance/src/Test.java')\n",
        "    uml_diagram = analyser.analyze_and_generate_uml()\n",
        "    print(uml_diagram)"
      ],
      "metadata": {
        "id": "wPi9pnO964cU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53a8657c-54b7-47b2-9c10-7396c940c5da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "@startuml\n",
            "@enduml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the fine tuning of a pre-trained Hugging Face model. However, it does require some login information for Hugging Face Account (I've left a passable token here that I will update later that should work \"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\"), and a wand token that will  be updated to not work later. Be warned it takes a decent amount of GPU space to train this shardered version in a reasonable amount of time."
      ],
      "metadata": {
        "id": "lfraqL987Y__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q trl xformers wandb datasets einops sentencepiece\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging, TextStreamer\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "import os, torch, wandb, platform, warnings\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from huggingface_hub import notebook_login\n",
        "import transformers\n",
        "transformers.logging.set_verbosity_debug()\n",
        "\n",
        "# This is the second version of the project which uses the Mistral LLM\n",
        "# For the purpose of understanding code\n",
        "base_model = 'filipealmeida/Mistral-7B-Instruct-v0.1-sharded'\n",
        "dataset_name, new_model = \"gathnex/Gath_baize\", \"gathnex/Gath_mistral_7b\"\n",
        "\n",
        "\n",
        "# Loading a Gath_baize dataset, which is a well known dataset with some code\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "dataset[\"chat_sample\"][0]\n",
        "\n",
        "# Load base model(Mistral 7B)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit= True,\n",
        "    bnb_4bit_quant_type= \"nf4\",\n",
        "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant= False,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    token=\"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\",\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
        "model.config.pretraining_tp = 1\n",
        "model.gradient_checkpointing_enable()\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_eos_token = True\n",
        "tokenizer.add_bos_token, tokenizer.add_eos_token\n",
        "\n",
        "wandb.login(key = \"fd00e65c7af38c0611779a7bb5f2a9b93c96f303\")\n",
        "run = wandb.init(project='Fine tuning mistral 7B', job_type=\"training\", anonymous=\"allow\")\n",
        "\n",
        "print(\"hi\",torch.cuda.memory_summary())\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "peft_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",
        "    )\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "\n",
        "print(\"hi\",torch.cuda.memory_summary())\n",
        "# Training Arguments\n",
        "# Hyperparameters should beadjusted based on the hardware you using\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir= \"./results\",\n",
        "    num_train_epochs= 1,\n",
        "    per_device_train_batch_size= 4,\n",
        "    gradient_accumulation_steps= 4,\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    save_steps= 1000,\n",
        "    logging_steps= 10,\n",
        "    learning_rate= 2e-4,\n",
        "    gradient_checkpointing=True,\n",
        "    weight_decay= 0.001,\n",
        "    fp16= True,\n",
        "    bf16= False,\n",
        "    max_grad_norm= 0.3,\n",
        "    max_steps= -1,\n",
        "    warmup_ratio= 0.1,\n",
        "    group_by_length= True,\n",
        "    lr_scheduler_type= \"constant\",\n",
        "    report_to=\"wandb\"\n",
        ")\n",
        "print(torch.cuda.memory_summary())\n",
        "# Setting sft parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length= None,\n",
        "    dataset_text_field=\"chat_sample\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing= False,\n",
        ")\n",
        "print(torch.cuda.memory_summary())\n",
        "trainer.train()\n",
        "print(torch.cuda.memory_summary())\n",
        "# Save the fine-tuned model\n",
        "trainer.model.save_pretrained(new_model)\n",
        "wandb.finish()\n",
        "model.config.use_cache = True\n",
        "model.eval()\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import Trainer\n",
        "\n",
        "# Assuming 'trainer' is your Trainer instance and 'tokenizer' is your tokenizer instance\n",
        "# Login to Hugging Face Hub (this will require you to input your access token)\n",
        "notebook_login()\n",
        "\n",
        "# Push the model to the Hub\n",
        "trainer.model.push_to_hub(\"gathnex/Gath_mistral_7b\")\n",
        "tokenizer.push_to_hub(\"gathnex/Gath_mistral_7b\")\n",
        "def stream(user_prompt):\n",
        "    runtimeFlag = \"cuda:0\"\n",
        "    system_prompt = 'The conversation between Human and AI assisatance named Gathnex\\n'\n",
        "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "\n",
        "    prompt = f\"{system_prompt}{B_INST}{user_prompt.strip()}\\n{E_INST}\"\n",
        "\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
        "\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=200)\n",
        "\n",
        "stream(\"Explain large language models\")\n",
        "# Clear the memory footprint\n",
        "del model, trainer\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Reload the base model\n",
        "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model, low_cpu_mem_usage=True,\n",
        "    return_dict=True,torch_dtype=torch.bfloat16,\n",
        "    device_map= {\"\": 0})\n",
        "model = PeftModel.from_pretrained(base_model_reload, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "#push the model to hub\n",
        "model.push_to_hub(new_model, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "XxqREt3C7YWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This uses a smaller AI and only can handle Java files,\n",
        "Run the Analyser test to see console outputs"
      ],
      "metadata": {
        "id": "WGr6Rjkbc82U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import AutoModelForQuestionAnswering, \\\n",
        "    AutoModelForSequenceClassification, \\\n",
        "    AutoTokenizer, \\\n",
        "    AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "import sentencepiece\n",
        "# import logging\n",
        "\n",
        "# import protobuf\n",
        "from transformers import file_utils\n",
        "\n",
        "# Access the default cache directory used by the transformers library\n",
        "cache_dir = file_utils.default_cache_path\n",
        "\n",
        "print(\"Transformers default cache directory:\", cache_dir)\n",
        "# logger = logging.getLogger('transformers.file_utils')\n",
        "# logger.setLevel(logging.DEBUG)\n",
        "class Analyser:\n",
        "    def analyse(self, root_directory):\n",
        "        model_name = 'distilbert-base-cased'\n",
        "        print(\"Analyser current working directory:\", os.getcwd())\n",
        "        files = self.list_files(root_directory)\n",
        "        for file_path in files:\n",
        "            print(file_path)\n",
        "            self.evaluate_file(model_name, file_path)\n",
        "\n",
        "    def list_files(self, root_dir):\n",
        "        # TODO exclude everything in .gitignore\n",
        "        excluded_directories = ['.git', '.idea', '.settings', '.mvn', 'target']\n",
        "        excluded_file_types = ['.gitattributes', '.md', '.class']\n",
        "        file_list = []\n",
        "        for root, dirs, files in os.walk(root_dir):\n",
        "            if any(excluded_dir in root for excluded_dir in excluded_directories):\n",
        "                continue\n",
        "            for file_name in files:\n",
        "                # Check if file type should be excluded\n",
        "                if any(file_name.endswith(extension) for extension in excluded_file_types):\n",
        "                    continue\n",
        "                file_path = os.path.join(root, file_name)\n",
        "                file_list.append(file_path)\n",
        "        return file_list\n",
        "\n",
        "    def evaluate_file(self, model_name, file_path):\n",
        "        # Load pre-trained model and tokenizer\n",
        "        # see https://huggingface.co/lintang/pile-t5-large-codexglue\n",
        "        print(\"loading model\")\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(\"lintang/pile-t5-large-codexglue\")\n",
        "        print(\"got model:\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"lintang/pile-t5-large-codexglue\")\n",
        "        print(\"tokenized\")\n",
        "\n",
        "        # Read the content of the file\n",
        "        with open(file_path, 'r') as file:\n",
        "            try:\n",
        "                code = file.read()\n",
        "                try:\n",
        "                    # Tokenize the code\n",
        "                    inputs = tokenizer(code, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
        "                    print(\"inputs:\", inputs)\n",
        "                    # Generate descriptive text\n",
        "                    outputs = model.generate(**inputs)\n",
        "                    print(\"outputs:\", outputs)\n",
        "                    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                    print(\"Descriptive text:\", decoded_output)\n",
        "                    print(\"----------------------------------\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(\"An error occurred in tokenizing:\", e)\n",
        "\n",
        "            except Exception as e:\n",
        "                decoded_output = \"error occurred\"\n",
        "                print(\"couldn't parse \" + file.name, e)\n",
        "\n",
        "        return decoded_output\n"
      ],
      "metadata": {
        "id": "azrIQupadG9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e16a940-61f5-41b3-cdde-9edaacc27988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers default cache directory: /root/.cache/huggingface/hub\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "# from google.colab import files\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# uploaded = files.upload()\n",
        "# Assuming Analyser class is already defined in another cell\n",
        "class TestAnalyzer(unittest.TestCase):\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "        cls.script_directory = '/content'  # Adjust path if necessary\n",
        "        class_under_test_directory = os.path.join(cls.script_directory, '..')\n",
        "        os.chdir(class_under_test_directory)\n",
        "        print(\"setting up\")\n",
        "        cls.analyser = Analyser()\n",
        "\n",
        "    @classmethod\n",
        "    def tearDownClass(cls):\n",
        "        cls.analyser = None\n",
        "\n",
        "    def test_analyse(self):\n",
        "        directory_path = '/content/drive/My Drive/londontube-main/londontube-main/src/'  # Adjust the path to where you have test data in Colab\n",
        "        self.analyser.analyse(directory_path)\n",
        "\n",
        "# Execute the tests\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=[''], exit=False)"
      ],
      "metadata": {
        "id": "lxF7CqHvdH6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "142cfc31-7331-4a96-f96b-84f9958277da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "setting up\n",
            "Analyser current working directory: /\n",
            "/content/drive/My Drive/londontube-main/londontube-main/src/main/resources/application.properties\n",
            "loading model\n",
            "got model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized\n",
            "inputs: {'input_ids': tensor([[    2, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100]]), 'attention_mask': tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "outputs: tensor([[    0,  7106,   263, 29871, 29941, 29928,     2]])\n",
            "Descriptive text: Return a 3D\n",
            "----------------------------------\n",
            "/content/drive/My Drive/londontube-main/londontube-main/src/main/java/uk/co/emma/londontube/LondontubeApplication.java\n",
            "loading model\n",
            "got model:\n",
            "tokenized\n",
            "inputs: {'input_ids': tensor([[ 3577, 18293, 29889,  1111, 29889,   331,   655, 29889, 29880,   898,\n",
            "           609,  4003, 29936,     0,  5215,  1638, 29889,  6688, 29889,  4777,\n",
            "         29889, 19634,  4873, 29936,     0,  5215,  1638, 29889,  6688, 29889,\n",
            "          4777, 29889,  6921, 17591, 29889, 19634, 20967,  4873, 29936,     0,\n",
            "         29992, 19634, 20967,  4873,     0,  3597,   770,  3621,   609,  4003,\n",
            "          4873,   426,     0,  3597,  2294,  1780,  1667, 29898,  1231,  2636,\n",
            "          6389, 29897,   426,     0, 19634,  4873, 29889,  3389, 29898, 26682,\n",
            "           609,  4003,  4873, 29889,  1990, 29892,  6389,   416,     0, 29913,\n",
            "             0, 29913,     2, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
            "outputs: tensor([[   0, 7525,  278, 3621,  609, 4003, 2280,  869,    2]])\n",
            "Descriptive text: Run the Londontube application.\n",
            "----------------------------------\n",
            "/content/drive/My Drive/londontube-main/londontube-main/src/main/java/uk/co/emma/londontube/builder/TubeBuilder.java\n",
            "loading model\n",
            "got model:\n",
            "tokenized\n",
            "inputs: {'input_ids': tensor([[ 3577, 18293, 29889,  1111, 29889,   331,   655, 29889, 29880,   898,\n",
            "           609,  4003, 29889, 16409, 29936,     0,  5215, 18293, 29889,  1111,\n",
            "         29889,   331,   655, 29889, 29880,   898,   609,  4003, 29889,  1129,\n",
            "          2212, 29889, 22814, 29936,     0,  5215, 18293, 29889,  1111, 29889,\n",
            "           331,   655, 29889, 29880,   898,   609,  4003, 29889,  1129,  2212,\n",
            "         29889, 13425, 29936,     0,  3597,   770,   323,  4003,  5627,   426,\n",
            "             0,  9053,   323,  4003,   278, 13425,   353,  1870, 29936,     0,\n",
            "          3597,   323,  4003,  5627,   580,   426,     0,  1552, 13425,   353,\n",
            "           716,   323,  4003,   890,     0,   458,  4150, 29915, 29879,   988,\n",
            "           591,  1033,  1925,   777,   775,   304, 10563,   278, 16355,   515,\n",
            "           278,  2566,     0,   458,   392,   769,   671,   278,   848,   304,\n",
            "         19450,   599,  1438,  3618,     0,   458,  2622,  5073, 29918,   978,\n",
            "           515,  5073,     0, 22814,  5073, 29909,   353,   716, 12039,   703,\n",
            "         29909,  1496,     0, 22814,  5073, 29933,   353,   716, 12039,   703,\n",
            "         29933,  1496,     0, 12039,  5073, 29907,   353,   716, 12039,   703,\n",
            "         29907,  1496,     0, 12039,  5073, 29928,   353,   716, 12039,   703,\n",
            "         29928,  1496,     0, 12039,  5073, 29923,   353,   716, 12039,   703,\n",
            "         29923,  1496,     0, 12039,  5073, 29943,   353,   716, 12039,   703,\n",
            "         29943,  1496,     0,   849,  2622,  1024, 29892,  5418,   515, 20114,\n",
            "         29918, 19569,   408,     0,   849,  7122,  5073,   269,   373,   263,\n",
            "         29889,   978,   353,   408, 29889,   978,     0,   849,  3062,   269,\n",
            "         29889,   978,   353,  9872, 19569,  1024,   515,   278,  6475,  2038,\n",
            "         11903,     0,  5073, 29909, 29889,  1202,  3253, 29926, 16648, 22814,\n",
            "         29898, 19569, 29933, 29892, 29871, 29896, 29900,   416,     0,  5073,\n",
            "         29909, 29889,  1202,  3253, 29926, 16648, 22814, 29898, 19569, 29907,\n",
            "         29892, 29871, 29896, 29945,   416,     0,  5073, 29933, 29889,  1202,\n",
            "          3253, 29926, 16648, 22814, 29898, 19569, 29928, 29892, 29871, 29896,\n",
            "         29906,   416,     0,  5073, 29933, 29889,  1202,  3253, 29926, 16648,\n",
            "         22814, 29898, 19569, 29943, 29892, 29871, 29896, 29945,   416,     0,\n",
            "          5073, 29907, 29889,  1202,  3253, 29926, 16648, 22814, 29898, 19569,\n",
            "         29923, 29892, 29871, 29896, 29900,   416,     0,  5073, 29928, 29889,\n",
            "          1202,  3253, 29926, 16648, 22814, 29898, 19569, 29923, 29892, 29871,\n",
            "         29906,   416,     0,  5073, 29928, 29889,  1202,  3253, 29926, 16648,\n",
            "         22814, 29898, 19569, 29943, 29892, 29871, 29896,   416,     0,  5073,\n",
            "         29943, 29889,  1202,  3253, 29926, 16648, 22814, 29898, 19569, 29923,\n",
            "         29892, 29871, 29945,   416,     0,   278, 13425, 29889,  1202, 22814,\n",
            "         29898, 19569, 29909,   416,     0,   278, 13425, 29889,  1202, 22814,\n",
            "         29898, 19569, 29933,   416,     0,   278, 13425, 29889,  1202, 22814,\n",
            "         29898, 19569, 29907,   416,     0,   278, 13425, 29889,  1202, 22814,\n",
            "         29898, 19569, 29928,   416,     0,   278, 13425, 29889,  1202, 22814,\n",
            "         29898, 19569, 29923,   416,     0,   278, 13425, 29889,  1202, 22814,\n",
            "         29898, 19569, 29943,   416,     0, 29913,     0,  3597,   323,  4003,\n",
            "          2048,   580,   426,     0,  2457,   278, 13425, 29936,     0, 29913,\n",
            "             0, 29913,     2, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
            "outputs: tensor([[    0,  8878,   263,   323,  4003,  1203,   515,   278,  2183,  1051,\n",
            "           310, 16355,   869,     2]])\n",
            "Descriptive text: Build a Tube object from the given list of stations.\n",
            "----------------------------------\n",
            "/content/drive/My Drive/londontube-main/londontube-main/src/main/java/uk/co/emma/londontube/pojo/Station.java\n",
            "loading model\n",
            "got model:\n",
            "tokenized\n",
            "inputs: {'input_ids': tensor([[ 3577, 18293, 29889,  1111, 29889,   331,   655, 29889, 29880,   898,\n",
            "           609,  4003, 29889,  1129,  2212, 29936,     0,  5215,  2115, 29889,\n",
            "          4422, 29889, 27824, 29936,     0,  5215,  2115, 29889,  4422, 29889,\n",
            "          6595,   287,  1293, 29936,     0,  5215,  2115, 29889,  4422, 29889,\n",
            "          1293, 29936,     0,  5215,  2115, 29889,  4422, 29889,  3388, 29936,\n",
            "             0,  3597,   770, 12039,   426,     0,  9053,  1714,  1024, 29936,\n",
            "             0,  9053,  8102,  1178, 29936,     0,  9053,  1714,  1196, 29936,\n",
            "           849, 29881,   348,  1217,   565,   591,   817,   445,     0,  9053,\n",
            "          2391, 29966, 22814, 29958,  3273,   342,  2605,   353,   716, 28547,\n",
            "          1293, 27258,     0,   458, 19244,   515,  3517,  5073, 29973, 11905,\n",
            "           304,  1554,  2289,  4802,     0,  9053,  8102,  5418,   353,  8102,\n",
            "         29889, 12648, 29918, 19143, 29936, 29871,     0,   849,   497,   278,\n",
            "          7389, 20114, 16355,   304,   445,   697,   411,   278,  5418,  1546,\n",
            "             0,  9053,  7315, 29966, 22814, 29892,  8102, 29958, 20114,   855,\n",
            "           800,   353,   716, 23073, 27258,     0,   458, 27821,     0,  3597,\n",
            "         12039, 29898,  1231,  1024, 29897,   426,     0,  1366, 29889,   978,\n",
            "           353,  1024, 29936,     0, 29913,     0,  3597,  1780,   788,  3253,\n",
            "         29926, 16648, 22814, 29898, 22814,  5073, 29892,  8102,  5418, 29897,\n",
            "           426,     0, 26859, 16648,   855,   800, 29889,   649, 29898, 19569,\n",
            "         29892,  5418,   416,   849, 19244,   515,  6257,  5073,     0, 29913,\n",
            "             0,  3597,  1714,   679,  1170,   580,   426,     0,  2457,  1024,\n",
            "         29936,     0, 29913,     0,  3597,  1780,   731,  1170, 29898,  1231,\n",
            "          1024, 29897,   426,     0,  1366, 29889,   978,   353,  1024, 29936,\n",
            "             0, 29913,     0,  3597,  8102,   679,  1204,   580,   426,     0,\n",
            "          2457,  1178, 29936,     0, 29913,     0,  3597,  1780,   731,  1204,\n",
            "         29898,  7798,  1178, 29897,   426,     0,  1366, 29889,   333,   353,\n",
            "          1178, 29936,     0, 29913,     0,  3597,  1714,   679,  3542,   580,\n",
            "           426,     0,  2457,  1196, 29936,     0, 29913,     0,  3597,  1780,\n",
            "           731,  3542, 29898,  1231,  1196, 29897,   426,     0,  1366, 29889,\n",
            "          1220,   353,  1196, 29936,     0, 29913,     0,  3597,  2391, 29966,\n",
            "         22814, 29958,   679, 21322,   342,  2605,   580,   426,     0,  2457,\n",
            "          3273,   342,  2605, 29936,     0, 29913,     0,  3597,  1780,   731,\n",
            "         21322,   342,  2605, 29898,  1293, 29966, 22814, 29958,  3273,   342,\n",
            "          2605, 29897,   426,     0,  1366, 29889, 12759,   342,  2605,   353,\n",
            "          3273,   342,  2605, 29936,     0, 29913,     0,  3597,  8102,   679,\n",
            "         27469,   580,   426,     0,  2457,  5418, 29936,     0, 29913,     0,\n",
            "          3597,  1780,   731, 27469, 29898,  7798,  5418, 29897,   426,     0,\n",
            "          1366, 29889, 19244,   353,  5418, 29936,     0, 29913,     0,  3597,\n",
            "          7315, 29966, 22814, 29892,  8102, 29958,   679,  3253, 29926, 16648,\n",
            "           855,   800,   580,   426,     0,  2457, 20114,   855,   800, 29936,\n",
            "             0, 29913,     0,  3597,  1780,   731,  3253, 29926, 16648,   855,\n",
            "           800, 29898,  3388, 29966, 22814, 29892,  8102, 29958, 20114,   855,\n",
            "           800, 29897,   426,     0,  1366, 29889, 26859, 16648,   855,   800,\n",
            "           353, 20114,   855,   800, 29936,     0, 29913,     0, 29913,     2,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
            "outputs: tensor([[   0, 3462,  263, 5073,  304,  278, 2910,  869,    2]])\n",
            "Descriptive text: Add a station to the map.\n",
            "----------------------------------\n",
            "/content/drive/My Drive/londontube-main/londontube-main/src/main/java/uk/co/emma/londontube/pojo/Tube.java\n",
            "loading model\n",
            "got model:\n",
            "tokenized\n",
            "inputs: {'input_ids': tensor([[ 3577, 18293, 29889,  1111, 29889,   331,   655, 29889, 29880,   898,\n",
            "           609,  4003, 29889,  1129,  2212, 29936,     0,  5215,  2115, 29889,\n",
            "          4422, 29889, 10438,  2697, 29936,     0,  5215,  2115, 29889,  4422,\n",
            "         29889, 20277, 29936,     0,  5215,  2115, 29889,  4422, 29889,  2697,\n",
            "         29936,     0,  3597,   770,   323,  4003,   426,     0,  9053,  3789,\n",
            "         29966, 22814, 29958, 16355,   353,   716, 11874,  2697, 27258,     0,\n",
            "          3597,  1780,   788, 22814, 29898, 22814,  5073, 29897,   426,     0,\n",
            "           303,   800, 29889,  1202, 29898, 19569,   416,     0, 29913,     0,\n",
            "          3597,  3789, 29966, 22814, 29958,   679,   855,   800,   580,   426,\n",
            "             0,  2457, 16355, 29936,     0, 29913,     0,  3597,  1780,   731,\n",
            "           855,   800, 29898,  2697, 29966, 22814, 29958, 16355, 29897,   426,\n",
            "             0,  1366, 29889,   303,   800,   353, 16355, 29936,     0, 29913,\n",
            "             0,   458,  1366,  1033,   367,   263,  4576,  2346,   856,     0,\n",
            "          3597, 12039,   679, 22814, 29898,  1231,  1024, 29897,   426,     0,\n",
            "         20277, 29966, 22814, 29958,   372, 29878,   353,   679,   855,   800,\n",
            "          2141, 17609,   890,     0, 22814,   445, 22814,   353,  1870, 29936,\n",
            "             0,  8000, 29898,   277, 29878, 29889,  5349,  9190,  3101,   426,\n",
            "             0,  1366, 22814,   353,   372, 29878, 29889,  4622,   890,     0,\n",
            "           361,   313,  1366, 22814, 29889, 19629,  2141, 10954, 29898,   978,\n",
            "           876,   426,     0,  2457,   445, 22814, 29936,     0, 29913,     0,\n",
            "         29913,     0,  2457,  1870, 29936,   849, 26680, 29876, 29915, 29873,\n",
            "          1284,   372,     0, 29913,     0, 29913,     2, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
            "outputs: tensor([[    0, 16969,   263,   731,   310, 16355,   363,   278,  2183,  1024,\n",
            "           869,     2]])\n",
            "Descriptive text: Returns a set of stations for the given name.\n",
            "----------------------------------\n",
            "/content/drive/My Drive/londontube-main/londontube-main/src/main/java/uk/co/emma/londontube/service/JourneyFinder.java\n",
            "loading model\n",
            "got model:\n",
            "tokenized\n",
            "inputs: {'input_ids': tensor([[ 3577, 18293, 29889,  1111, 29889,   331,   655, 29889, 29880,   898,\n",
            "           609,  4003, 29889,  5509, 29936,     0,  5215,  2115, 29889,  4422,\n",
            "         29889, 10438,  2697, 29936,     0,  5215,  2115, 29889,  4422, 29889,\n",
            "          6595,   287,  1293, 29936,     0,  5215,  2115, 29889,  4422, 29889,\n",
            "          3388, 29889,  9634, 29936,     0,  5215,  2115, 29889,  4422, 29889,\n",
            "          2697, 29936,     0,  5215, 18293, 29889,  1111, 29889,   331,   655,\n",
            "         29889, 29880,   898,   609,  4003, 29889,  1129,  2212, 29889, 22814,\n",
            "         29936,     0,   458,  5215, 18293, 29889,  1111, 29889,   331,   655,\n",
            "         29889, 29880,   898,   609,  4003, 29889,  1129,  2212, 29889, 13425,\n",
            "         29936,     0,  3597,   770,   435,   473,  3801, 29943,  4995,   426,\n",
            "             0,  3597,  1780,  8147, 21322,  2605,  4591,  4763, 29898, 22814,\n",
            "          1369, 29897,   426,     0,  2962, 29889,   842, 27469, 29898, 29900,\n",
            "           416,   849,   271,  1749,  6257,  1298, 29892,  1749,  5418,   338,\n",
            "          5225,     0,   458,   517,  3013,  5702,   310,   607, 16355,   591,\n",
            "           505,   322,   505,   451,  3447, 19030,     0,  2697, 29966, 22814,\n",
            "         29958,   443,  9915,   839,   855,   800,   353,   716, 11874,  2697,\n",
            "         27258,     0,  2697, 29966, 22814, 29958, 17141,   855,   800,   353,\n",
            "           716, 11874,  2697, 27258,     0,   458,  2962,   278, 17983,   472,\n",
            "           278,  6763,   856,     0,   348,  9915,   839,   855,   800, 29889,\n",
            "          1202, 29898,  2962,   416, 29871,     0,  8000,  5384,   348,  9915,\n",
            "           839,   855,   800, 29889, 24326,  3101,   426,     0, 22814,  1857,\n",
            "         22814,   353,   679, 22814,  3047, 21322,   342, 27469, 29898,   348,\n",
            "          9915,   839,   855,   800,   416,     0,   458,  1366,   338,  1286,\n",
            "           278,  1857,  5073,   393,   591,   526,  6161,  1218,     0,   348,\n",
            "          9915,   839,   855,   800, 29889,  5992, 29898,  3784, 22814,   416,\n",
            "             0,  1454, 29898,  9634, 29966, 22814, 29892,  8102, 29958, 20114,\n",
            "         22814,  9634, 29901,  1857, 22814, 29889,   657,  3253, 29926, 16648,\n",
            "           855,   800,  2141,  8269,  2697,  3101,   426,     0, 22814, 20114,\n",
            "         22814,   353, 20114, 22814,  9634, 29889,   657,  2558,   890,     0,\n",
            "          7798,  7636, 22676,   353, 20114, 22814,  9634, 29889, 23433,   890,\n",
            "             0,   361,  5384,  9915,   839,   855,   800, 29889, 11516, 29898,\n",
            "         26859, 16648, 22814,   876,   426,     0, 15807,   403,  8140, 12539,\n",
            "         27469, 29898, 26859, 16648, 22814, 29892,  7636, 22676, 29892,  1857,\n",
            "         22814,   416,   849, 16406,  1333,  1854,   825,   445,  7636, 22676,\n",
            "          2869,   338, 29973,  3508, 29915, 29873,   372,   278,  1021,   408,\n",
            "         20114, 22814, 29889,   657, 27469, 29973,     0,   458,  1202,   445,\n",
            "           304,   278,   731,   304, 14707,  2446,   856,     0,   348,  9915,\n",
            "           839,   855,   800, 29889,  1202, 29898, 26859, 16648, 22814,   416,\n",
            "             0, 29913,     0, 29913,     0,   458,   497,  2309,   411,   445,\n",
            "           697,   856,     0,  9915,   839,   855,   800, 29889,  1202, 29898,\n",
            "          3784, 22814,   416, 29871,     0, 29913,     0, 29913,     0,   458,\n",
            "           510,   862,   267,   278,  3935,  5418,   411,   278, 15141, 12833,\n",
            "           697, 29871,     0,   458,  8000,  1494,   278, 15141,  3902,  4395,\n",
            "          2224,     0,   458,  1366,   338,   263,  2586, 16051,   541,   278,\n",
            "          1820,  8424,   856,     0,  9053,  1780,  8147,  8140, 12539, 27469,\n",
            "         29898, 22814, 17983, 22814, 29892,  8102,  7636, 22676, 29892, 12039,\n",
            "          2752, 22814, 29897,   426,     0,  7798,  2752, 22814, 27469,   353,\n",
            "          2752, 22814, 29889,   657, 27469,   890,     0,   361,   313,  4993,\n",
            "         22814, 27469,   718,  7636, 22676,   529, 17983, 22814, 29889,   657,\n",
            "         27469,  3101,   426,     0, 24219,   362, 22814, 29889,   842, 27469,\n",
            "         29898,  4993, 22814, 27469,   718,  7636, 22676,   416,     0,  6595,\n",
            "           287,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "outputs: tensor([[    0, 20535,   403,   278,  3273,   342,  2224,   515,  1369,   304,\n",
            "         12551,   869,     2]])\n",
            "Descriptive text: Calculate the shortest path from start to destination.\n",
            "----------------------------------\n",
            "/content/drive/My Drive/londontube-main/londontube-main/src/test/java/uk/co/emma/londontube/LondontubeApplicationTests.java\n",
            "loading model\n",
            "got model:\n",
            "tokenized\n",
            "inputs: {'input_ids': tensor([[ 3577, 18293, 29889,  1111, 29889,   331,   655, 29889, 29880,   898,\n",
            "           609,  4003, 29936,     0,  5215,  1638, 29889, 18491, 29889, 29926,\n",
            "           786,  1524, 29889,  2754, 29889,  3057, 29936,     0,  5215,  1638,\n",
            "         29889,  6688, 29889,  4777, 29889,  1688, 29889,  4703, 29889, 19634,\n",
            "         20967,  3057, 29936,     0, 29992, 19634, 20967,  3057,     0,  1990,\n",
            "          3621,   609,  4003,  4873, 24376,   426,     0, 29992,  3057,     0,\n",
            "          5405,  3030,  5896, 29879,   580,   426,     0,  3924, 29889,   449,\n",
            "         29889,  5248,   703, 12199,  1496,     0, 29913,     0, 29913,     2,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100, 32100,\n",
            "         32100, 32100]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
            "outputs: tensor([[    0,  4321, 29879,   363,  3621,   609,  4003,  4873,   869,     2]])\n",
            "Descriptive text: Tests for LondontubeApplication.\n",
            "----------------------------------\n",
            "/content/drive/My Drive/londontube-main/londontube-main/src/test/java/uk/co/emma/londontube/service/JourneyFinderTest.java\n",
            "loading model\n",
            "got model:\n",
            "tokenized\n",
            "inputs: {'input_ids': tensor([[ 3577, 18293, 29889,  1111, 29889,   331,   655, 29889, 29880,   898,\n",
            "           609,  4003, 29889,  5509, 29936,     0,  5215,  2294,  1638, 29889,\n",
            "         18491, 29889, 14697, 29889,  9294,  5574, 29936,     0,  5215,  2115,\n",
            "         29889,  4422, 29889,  2588, 29879, 29936,     0,  5215,  2115, 29889,\n",
            "          4422, 29889,  1293, 29936,     0,  5215,  1638, 29889, 18491, 29889,\n",
            "          3057, 29936,     0,  5215, 18293, 29889,  1111, 29889,   331,   655,\n",
            "         29889, 29880,   898,   609,  4003, 29889, 16409, 29889, 13425,  5627,\n",
            "         29936,     0,  5215, 18293, 29889,  1111, 29889,   331,   655, 29889,\n",
            "         29880,   898,   609,  4003, 29889,  1129,  2212, 29889, 22814, 29936,\n",
            "             0,  5215, 18293, 29889,  1111, 29889,   331,   655, 29889, 29880,\n",
            "           898,   609,  4003, 29889,  1129,  2212, 29889, 13425, 29936,     0,\n",
            "          3597,   770,   435,   473,  3801, 29943,  4995,  3057,   426,     0,\n",
            "         29992,  3057,     0,  3597,  1780,  8147, 21322,  2605,  4591,  4763,\n",
            "           580,   426,     0, 13425,   260,  4003,   353,   716,   323,  4003,\n",
            "          5627,  2141,  4282,   890,     0,  3924, 29889,   449, 29889,  5248,\n",
            "           703, 29967,   473,  3801, 29943,  4995,  1243,  1496,     0, 22814,\n",
            "          2943, 29909,   353,   260,  4003, 29889,   657, 22814,   703, 29909,\n",
            "          1496,     0, 22814,  2943, 29933,   353,   260,  4003, 29889,   657,\n",
            "         22814,   703, 29933,  1496,     0, 12039,  2943, 29907,   353,   260,\n",
            "          4003, 29889,   657, 22814,   703, 29907,  1496,     0, 12039,  2943,\n",
            "         29928,   353,   260,  4003, 29889,   657, 22814,   703, 29928,  1496,\n",
            "             0, 12039,  2943, 29923,   353,   260,  4003, 29889,   657, 22814,\n",
            "           703, 29923,  1496,     0, 12039,  2943, 29943,   353,   260,  4003,\n",
            "         29889,   657, 22814,   703, 29943,  1496,     0,   435,   473,  3801,\n",
            "         29943,  4995, 16342, 29943,  4995,   353,   716,   435,   473,  3801,\n",
            "         29943,  4995,   890, 29871,     0, 16342, 29943,  4995, 29889, 15807,\n",
            "           403, 21322,  2605,  4591,  4763, 29898,  3177, 29909,   416,     0,\n",
            "          2391, 29966, 22814, 29958,  3273,   342,  2605,  2831,  4247, 29933,\n",
            "           353,  4398, 29879, 29889,   294,  1293, 29898,  3177, 29909,   416,\n",
            "             0,  2391, 29966, 22814, 29958,  3273,   342,  2605,  2831,  4247,\n",
            "         29907,   353,  4398, 29879, 29889,   294,  1293, 29898,  3177, 29909,\n",
            "           416,     0,  2391, 29966, 22814, 29958,  3273,   342,  2605,  2831,\n",
            "          4247, 29928,   353,  4398, 29879, 29889,   294,  1293, 29898,  3177,\n",
            "         29909, 29892,  2943, 29933,   416,     0,  2391, 29966, 22814, 29958,\n",
            "          3273,   342,  2605,  2831,  4247, 29923,   353,  4398, 29879, 29889,\n",
            "           294,  1293, 29898,  3177, 29909, 29892,  2943, 29933, 29892,  2943,\n",
            "         29928,   416,     0,  2391, 29966, 22814, 29958,  3273,   342,  2605,\n",
            "          2831,  4247, 29943,   353,  4398, 29879, 29889,   294,  1293, 29898,\n",
            "          3177, 29909, 29892,  2943, 29933, 29892,  2943, 29928,   416,     0,\n",
            "           363,   313, 22814,  2943,   584,   260,  4003, 29889,   657,   855,\n",
            "           800,  3101,   426,     0,  4607,   313,  3177, 29889, 19629,  3101,\n",
            "           426,     0,  1206,   376, 29933,  1115,     0,  4974,  5574, 29898,\n",
            "          3177,     0,   869,   657, 21322,   342,  2605,   580,     0,   869,\n",
            "         10954, 29898, 12759,   342,  2605,  2831,  4247, 29933,  2483,     0,\n",
            "          2867, 29936,     0,  1206,   376, 29907,  1115,     0,  4974,  5574,\n",
            "         29898,  3177,     0,   869,   657, 21322,   342,  2605,   580,     0,\n",
            "           869, 10954, 29898, 12759,   342,  2605,  2831,  4247, 29907,  2483,\n",
            "             0,  2867, 29936,     0,  1206,   376, 29928,  1115,     0,  4974,\n",
            "          5574, 29898,  3177,     0,   869,   657, 21322,   342,  2605,   580,\n",
            "             0,   869, 10954, 29898, 12759,   342,  2605,  2831,  4247, 29928,\n",
            "          2483,     0,  2867, 29936,     0,  1206,   376, 29923,  1115,     0,\n",
            "          4974,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 153.849s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "outputs: tensor([[    0, 20535,   403,   278,  3273,   342,  2224,   515,  1369,   304,\n",
            "          1095,   310,   278,   260,  4003,   869,     2]])\n",
            "Descriptive text: Calculate the shortest path from start to end of the tube.\n",
            "----------------------------------\n"
          ]
        }
      ]
    }
  ]
}